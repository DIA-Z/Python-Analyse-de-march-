{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e3d1da",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a06af",
   "metadata": {},
   "source": [
    "Choisissez n'importe quelle page Produit sur le site de Books to Scrape. Écrivez un\n",
    "script Python qui visite cette page et en extrait les informations suivantes :\n",
    "\n",
    "- product_page_url\n",
    "- universal_ product_code (upc)\n",
    "- title\n",
    "- price_including_tax\n",
    "- price_excluding_tax\n",
    "- number_available\n",
    "- product_description\n",
    "- category\n",
    "- review_rating\n",
    "- image_url\n",
    "\n",
    "Écrivez les données dans un fichier CSV qui utilise les champs ci-dessus comme\n",
    "en-têtes de colonnes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f349309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\zizid\\anaconda3\\lib\\site-packages (4.9.3)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zizid\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324c92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Envoyer une requête HTTP à la page produit\n",
    "response = requests.get('http://books.toscrape.com/catalogue/the-black-maria_991/index.html')\n",
    "\n",
    "# Parser le contenu HTML de la page produit avec BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extraire les informations requises de la page produit\n",
    "product_page_url = 'http://books.toscrape.com/catalogue/the-black-maria_991/index.html'\n",
    "universal_product_code = soup.find('th', string='UPC').find_next_sibling('td').text\n",
    "title = soup.find('h1').text\n",
    "price_including_tax = soup.find('th', string='Price (incl. tax)').find_next_sibling('td').text\n",
    "price_excluding_tax = soup.find('th', string='Price (excl. tax)').find_next_sibling('td').text\n",
    "number_available = soup.find('th', string='Availability').find_next_sibling('td').text.strip()\n",
    "product_description = soup.find('div', {'id': 'product_description'}).find_next_sibling('p').text.strip()\n",
    "category = soup.find('ul', {'class': 'breadcrumb'}).find_all('a')[2].text\n",
    "review_rating = soup.find('p', {'class': 'star-rating'}).get('class')[1]\n",
    "image_url = soup.find('div', {'class': 'item active'}).find('img')['src']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023f8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire les données dans un fichier CSV\n",
    "import csv\n",
    "\n",
    "with open('product_info.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['product_page_url', 'universal_product_code', 'title', 'price_including_tax', \n",
    "                     'price_excluding_tax', 'number_available', 'product_description', 'category',\n",
    "                     'review_rating', 'image_url'])\n",
    "    writer.writerow([product_page_url, universal_product_code, title, price_including_tax, \n",
    "                     price_excluding_tax, number_available, product_description, category,\n",
    "                     review_rating, image_url])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711e522",
   "metadata": {},
   "source": [
    "# Phase 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c049d",
   "metadata": {},
   "source": [
    "Maintenant que vous avez obtenu les informations concernant un premier livre,\n",
    "vous pouvez essayer de récupérer toutes les données nécessaires pour toute une\n",
    "catégorie d'ouvrages.\n",
    "\n",
    "Choisissez n'importe quelle catégorie sur le site de Books to Scrape. Écrivez un\n",
    "script Python qui consulte la page de la catégorie choisie, et extrait l'URL de la\n",
    "page Produit de chaque livre appartenant à cette catégorie.\n",
    "\n",
    "Combinez cela avec le travail que vous avez déjà effectué dans la phase 1 afin\n",
    "d'extraire les données produit de tous les livres de la catégorie choisie, puis écrivez\n",
    "les données dans un seul fichier CSV.\n",
    "\n",
    "Remarque : certaines pages de catégorie comptent plus de 20 livres, qui sont\n",
    "donc répartis sur différentes pages (« pagination »). Votre application doit être\n",
    "capable de parcourir automatiquement les multiples pages si présentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378d91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Définir l'URL de la page de la catégorie choisie\n",
    "category_url = 'http://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
    "\n",
    "# Envoyer une requête GET à l'URL de la catégorie et récupérer le contenu HTML de la réponse\n",
    "response = requests.get(category_url)\n",
    "html = response.content\n",
    "\n",
    "# Parser le contenu HTML avec Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Trouver tous les liens de page produit dans la page de catégorie\n",
    "product_links = soup.select('.product_pod h3 a')\n",
    "\n",
    "# Créer une liste pour stocker les données de chaque livre\n",
    "books_data = []\n",
    "\n",
    "# Parcourir les liens de page produit et extraire les informations nécessaires pour chaque livre\n",
    "for link in product_links:\n",
    "    # Construire l'URL de la page produit à partir du lien relatif trouvé\n",
    "    product_url = 'http://books.toscrape.com/catalogue/' + link['href'].replace('../', '')\n",
    "    \n",
    "    # Envoyer une requête GET à l'URL de la page produit et récupérer le contenu HTML de la réponse\n",
    "    response = requests.get(product_url)\n",
    "    html = response.content\n",
    "    \n",
    "    # Parser le contenu HTML avec Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extraire les informations nécessaires de la page produit\n",
    "    product_page_url = product_url\n",
    "    title = soup.select_one('.product_main h1').text\n",
    "    td_element = soup.select_one('table tr:nth-of-type(1) td')\n",
    "    universal_product_code = td_element.text if td_element else None\n",
    "    td_element = soup.select_one('table tr:nth-of-type(2) td')\n",
    "    price_excluding_tax = td_element.text if td_element else None\n",
    "    td_element = soup.select_one('table tr:nth-of-type(3) td')\n",
    "    price_including_tax = td_element.text if td_element else None\n",
    "    td_element = soup.select_one('table tr:nth-of-type(5) td')\n",
    "    number_available = td_element.text if td_element else None\n",
    "    product_description = soup.select_one('#product_description + p').text\n",
    "    category = soup.select('.breadcrumb li')[2].text.strip()\n",
    "    td_element = soup.select_one('p.star-rating')\n",
    "    review_rating = td_element['class'][1] if td_element else None\n",
    "    img_element = soup.select_one('.item img')\n",
    "    image_url = 'http://books.toscrape.com/' + img_element['src'].replace('../', '')\n",
    "    \n",
    "    # Ajouter les informations du livre dans la liste\n",
    "    book_data = [product_page_url, universal_product_code, title, price_including_tax, \n",
    "                 price_excluding_tax, number_available, product_description, category,\n",
    "                 review_rating, image_url]\n",
    "    books_data.append(book_data)\n",
    "\n",
    "# Écrire les données de tous les livres dans un fichier CSV\n",
    "with open('mystery_books.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['product_page_url', 'universal_product_code', 'title', 'price_including_tax', \n",
    "                     'price_excluding_tax', 'number_available', 'product_description', 'category',\n",
    "                     'review_rating', 'image_url'])\n",
    "    writer.writerows(books_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16bba6",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a09c0c",
   "metadata": {},
   "source": [
    "Ensuite, étendez votre travail à l'écriture d'un script qui consulte le site de Books to Scrape, extrait toutes les catégories de livres disponibles, puis extrait les informations produit de tous les livres appartenant à toutes les différentes catégories. \n",
    "\n",
    "Vous devrez écrire les données dans un fichier CSV distinct pour chaque catégorie de livres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa1dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "\n",
    "# Fonction pour extraire les données d'un livre\n",
    "def get_book_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        title = soup.select_one('h1').text\n",
    "    except:\n",
    "        title = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        price_including_tax = soup.select_one('.price_color').text\n",
    "    except:\n",
    "        price_including_tax = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        price_excluding_tax = soup.select_one('table tr:nth-of-type(3) td').text\n",
    "    except:\n",
    "        price_excluding_tax = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        number_available = soup.select_one('table tr:nth-of-type(6) td').text.strip()\n",
    "    except:\n",
    "        number_available = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        product_description = soup.select_one('#product_description ~ p').text\n",
    "    except:\n",
    "        product_description = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        category = soup.select('.breadcrumb li:nth-of-type(3) a')[0].text.strip()\n",
    "    except:\n",
    "        category = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        review_rating_tag = soup.select_one('.product_page_rating p')['class'][1]\n",
    "        review_rating = review_rating_tag.replace('star-rating', '').strip()\n",
    "    except:\n",
    "        review_rating = 'N/A'\n",
    "    \n",
    "    book_data = {\n",
    "        'Title': title,\n",
    "        'Price (incl. tax)': price_including_tax,\n",
    "        'Price (excl. tax)': price_excluding_tax,\n",
    "        'Availability': number_available,\n",
    "        'Product Description': product_description,\n",
    "        'Category': category,\n",
    "        'Review Rating': review_rating\n",
    "    }\n",
    "    \n",
    "    return book_data\n",
    "\n",
    "# Récupération des liens de toutes les catégories\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "categories_links = soup.select('.side_categories a')\n",
    "categories_urls = [urljoin(base_url, link['href']) for link in categories_links]\n",
    "\n",
    "# Extraction des données de chaque livre pour chaque catégorie et écriture dans un fichier CSV distinct\n",
    "for category_url in categories_urls:\n",
    "    response = requests.get(category_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    category_name = soup.select('.page-header h1')[0].text.strip()\n",
    "    books_urls = [urljoin(category_url, link['href']) for link in soup.select('.product_pod h3 a')]\n",
    "    \n",
    "    with open(f'{category_name}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Title', 'Price (incl. tax)', 'Price (excl. tax)', 'Availability', 'Product Description', 'Category', 'Review Rating']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for book_url in books_urls:\n",
    "            book_data = get_book_data(book_url)\n",
    "            writer.writerow(book_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3381f6",
   "metadata": {},
   "source": [
    "Ce code va créer un fichier CSV distinct pour chaque catégorie de livres sur le site \"Books to Scrape\" et écrire les données de tous les livres appartenant à chaque catégorie dans leur propre fichier CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc100c",
   "metadata": {},
   "source": [
    "# Phase 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348ab75",
   "metadata": {},
   "source": [
    "Enfin, prolongez votre travail existant pour télécharger et enregistrer le fichier image de chaque page Produit que vous consultez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0160c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Liste des catégories\n",
    "categories_urls = [\n",
    "    'http://books.toscrape.com/catalogue/category/books/mystery_3/index.html',\n",
    "    'http://books.toscrape.com/catalogue/category/books/travel_2/index.html'\n",
    "]\n",
    "\n",
    "# Fonction pour récupérer les informations d'une page produit\n",
    "def get_product_information(product_url):\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_page_url = product_url\n",
    "    upc = soup.find('th', text='UPC').find_next_sibling('td').text\n",
    "    title = soup.find('h1').text.strip()\n",
    "    price_including_tax = soup.find('th', text='Price (incl. tax)').find_next_sibling('td').text\n",
    "    price_excluding_tax = soup.find('th', text='Price (excl. tax)').find_next_sibling('td').text\n",
    "    number_available = soup.find('th', text='Availability').find_next_sibling('td').text\n",
    "    product_description = soup.find('div', {'id': 'product_description'}).find_next_sibling('p').text\n",
    "    category = soup.find('ul', {'class': 'breadcrumb'}).find_all('a')[2].text.strip()\n",
    "    review_rating = soup.find('p', {'class': 'star-rating'})['class'][1]\n",
    "    image_url = soup.find('img')['src'].replace('../..', 'http://books.toscrape.com')\n",
    "    return [product_page_url, upc, title, price_including_tax, price_excluding_tax, number_available, product_description, category, review_rating, image_url]\n",
    "\n",
    "# Fonction pour récupérer toutes les URLs des pages produits d'une catégorie\n",
    "def get_category_product_urls(category_url):\n",
    "    product_urls = []\n",
    "    page_url = category_url\n",
    "    while True:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_links = soup.find_all('a', {'class': 'booktitle'})\n",
    "        for link in product_links:\n",
    "            product_url = link['href'].replace('../../../', 'http://books.toscrape.com/catalogue/')\n",
    "            product_urls.append(product_url)\n",
    "        next_button = soup.find('li', {'class': 'next'})\n",
    "        if next_button is None:\n",
    "            break\n",
    "        else:\n",
    "            page_url = category_url.replace('index.html', next_button.find('a')['href'])\n",
    "    return product_urls\n",
    "\n",
    "# Fonction pour enregistrer l'image d'une page produit\n",
    "def download_product_image(image_url, filepath):\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open(filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "\n",
    "# Parcours de toutes les catégories\n",
    "for category_url in categories_urls:\n",
    "    # Récupération des URLs des pages produits de la catégorie\n",
    "    product_urls = get_category_product_urls(category_url)\n",
    "\n",
    "    # Récupération des informations de chaque page produit\n",
    "    products_data = []\n",
    "    for product_url in product_urls:\n",
    "        product_data = get_product_information(product_url)\n",
    "        products_data.append(product_data)\n",
    "\n",
    "        # Téléchargement de l'image de la page produit\n",
    "        image_url = product_data[-1]\n",
    "        image_filename = image_url.split('/')[-1]\n",
    "        image_filepath = f'{category_url.split(\"/\")[-2]}_images/{image_filename}'\n",
    "        download_product_image(image_url, image_filepath)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9aa7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement des données dans un fichier CSV\n",
    "import csv\n",
    "\n",
    "# Les données à enregistrer dans le fichier CSV\n",
    "data = [\n",
    "    [\"product_page_url\", \"universal_product_code (upc)\", \"title\", \"price_including_tax\", \"price_excluding_tax\",\n",
    "     \"number_available\", \"product_description\", \"category\", \"review_rating\", \"image_url\"],\n",
    "    [\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\", \"a897fe39b1053632\",\n",
    "     \"A Light in the Attic\", \"£51.77\", \"£45.17\", \"1000\", \"It's hard to imagine a world without A Light in the Attic.\",\n",
    "     \"Poetry\", \"Three\", \"http://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg\"],\n",
    "    [\"http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\", \"90fa61229261140a\",\n",
    "     \"Tipping the Velvet\", \"£53.74\", \"£44.10\", \"20\", \"Erotic and absorbing, this is a truly remarkable debut novel.\",\n",
    "     \"Historical Fiction\", \"One\", \"http://books.toscrape.com/media/cache/da/54/da549f5cd5f74342c1b7e3f1ab8b1f1e.jpg\"]\n",
    "]\n",
    "\n",
    "# Enregistrement des données dans le fichier CSV\n",
    "with open(\"booksFinal.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc2b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Liste des catégories\n",
    "categories_urls = [    'http://books.toscrape.com/catalogue/category/books/mystery_3/index.html',    'http://books.toscrape.com/catalogue/category/books/travel_2/index.html']\n",
    "\n",
    "# Fonction pour récupérer les informations d'une page produit\n",
    "def get_product_information(product_url):\n",
    "    response = requests.get(product_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_page_url = product_url\n",
    "    upc = soup.find('th', text='UPC').find_next_sibling('td').text\n",
    "    title = soup.find('h1').text.strip()\n",
    "    price_including_tax = soup.find('th', text='Price (incl. tax)').find_next_sibling('td').text\n",
    "    price_excluding_tax = soup.find('th', text='Price (excl. tax)').find_next_sibling('td').text\n",
    "    number_available = soup.find('th', text='Availability').find_next_sibling('td').text\n",
    "    product_description = soup.find('div', {'id': 'product_description'}).find_next_sibling('p').text\n",
    "    category = soup.find('ul', {'class': 'breadcrumb'}).find_all('a')[2].text.strip()\n",
    "    review_rating = soup.find('p', {'class': 'star-rating'})['class'][1]\n",
    "    image_url = soup.find('img')['src'].replace('../..', 'http://books.toscrape.com')\n",
    "    return [product_page_url, upc, title, price_including_tax, price_excluding_tax, number_available, product_description, category, review_rating, image_url]\n",
    "\n",
    "# Fonction pour récupérer toutes les URLs des pages produits d'une catégorie\n",
    "def get_category_product_urls(category_url):\n",
    "    product_urls = []\n",
    "    page_url = category_url\n",
    "    while True:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_links = soup.find_all('a', {'class': 'booktitle'})\n",
    "        for link in product_links:\n",
    "            product_url = link['href'].replace('../../../', 'http://books.toscrape.com/catalogue/')\n",
    "            product_urls.append(product_url)\n",
    "        next_button = soup.find('li', {'class': 'next'})\n",
    "        if next_button is None:\n",
    "            break\n",
    "        else:\n",
    "            page_url = category_url.replace('index.html', next_button.find('a')['href'])\n",
    "    return product_urls\n",
    "\n",
    "# Fonction pour enregistrer l'image d'une page produit\n",
    "def download_product_image(image_url, filepath):\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    with open(filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "\n",
    "# Parcours de toutes les catégories\n",
    "for category_url in categories_urls:\n",
    "    # Création du dossier pour enregistrer les images\n",
    "    category_folder = category_url.split('/')[-2] + '_images'\n",
    "    os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "    # Récupération des URLs des pages produits de la catégorie\n",
    "    product_urls = get_category_product_urls(category_url)\n",
    "\n",
    "    # Récupération des informations de chaque page produit\n",
    "    products_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951abf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "# Fonction pour télécharger et enregistrer une image\n",
    "def save_image(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "# URL de la page d'accueil du site à scraper\n",
    "url = 'http://books.toscrape.com/'\n",
    "\n",
    "# Récupération du contenu HTML de la page d'accueil\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    # Création d'un objet BeautifulSoup à partir du contenu HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Récupération de tous les liens de catégorie\n",
    "    category_links = soup.select('div.side_categories > ul > li > ul > li > a')\n",
    "\n",
    "    # Boucle sur chaque lien de catégorie\n",
    "    for category_link in category_links:\n",
    "\n",
    "        # Récupération de l'URL de la catégorie\n",
    "        category_url = urllib.parse.urljoin(url, category_link['href'])\n",
    "\n",
    "        # Récupération du contenu HTML de la page de la catégorie\n",
    "        response = requests.get(category_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # Création d'un objet BeautifulSoup à partir du contenu HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Récupération de tous les liens de livre sur la page\n",
    "            book_links = soup.select('h3 > a')\n",
    "\n",
    "            # Boucle sur chaque lien de livre\n",
    "            for book_link in book_links:\n",
    "\n",
    "                # Récupération de l'URL de la page de produit\n",
    "                book_url = urllib.parse.urljoin(category_url, book_link['href'])\n",
    "\n",
    "                # Récupération du contenu HTML de la page de produit\n",
    "                response = requests.get(book_url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "\n",
    "                    # Création d'un objet BeautifulSoup à partir du contenu HTML\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Récupération de l'URL de l'image du livre\n",
    "                    image_url = urllib.parse.urljoin(book_url, soup.select_one('div.item > img')['src'])\n",
    "\n",
    "                    # Récupération du nom du fichier à partir de l'URL de l'image\n",
    "                    filename = os.path.basename(image_url)\n",
    "\n",
    "                    # Téléchargement et enregistrement de l'image\n",
    "                    save_image(image_url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d3a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
